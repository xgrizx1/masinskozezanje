{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import nltk\n",
    "# nltk.download()\n",
    "def load_dateset():\n",
    "    X, y = [], []\n",
    "    with open('domaci_data/train.csv', 'r', errors='ignore') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "        next(reader, None) # Skip header\n",
    "        for row in reader:\n",
    "            y.append(int(row[1]))\n",
    "            X.append(row[2])\n",
    "    return X, y\n",
    "tweet_strings_list, results_list = load_dateset()\n",
    "\n",
    "all_tweets = \"\".join(tweet_strings_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "tokenz = word_tokenize(all_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "def clean_tokenz(tokenz):\n",
    "    \n",
    "    stopwords_punctuation_list = set(stopwords.words('english')).union(set(punctuation)).union(['--','...'])\n",
    "    words_clean = [w.lower() for w in tokenz if w not in stopwords_punctuation_list]\n",
    "    return words_clean\n",
    "words_clean = clean_tokenz(tokenz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 66)\t1\n",
      "  (1, 222)\t1\n",
      "  (2, 0)\t1\n",
      "  (4, 145)\t1\n",
      "  (5, 112)\t1\n",
      "  (6, 194)\t1\n",
      "  (7, 94)\t1\n",
      "  (8, 3)\t1\n",
      "  (10, 69)\t1\n",
      "  (11, 113)\t1\n",
      "  (12, 243)\t1\n",
      "  (13, 117)\t1\n",
      "  (14, 198)\t1\n",
      "  (15, 36)\t1\n",
      "  (16, 187)\t1\n",
      "  (17, 90)\t1\n",
      "  (18, 129)\t1\n",
      "  (19, 102)\t1\n",
      "  (20, 47)\t1\n",
      "  (21, 106)\t1\n",
      "  (22, 29)\t1\n",
      "  (23, 54)\t1\n",
      "  (24, 83)\t1\n",
      "  (25, 185)\t1\n",
      "  (27, 120)\t1\n",
      "  :\t:\n",
      "  (301, 83)\t1\n",
      "  (303, 237)\t1\n",
      "  (305, 157)\t1\n",
      "  (306, 146)\t1\n",
      "  (307, 146)\t1\n",
      "  (308, 146)\t1\n",
      "  (309, 141)\t1\n",
      "  (310, 171)\t1\n",
      "  (311, 189)\t1\n",
      "  (312, 56)\t1\n",
      "  (313, 202)\t1\n",
      "  (314, 25)\t1\n",
      "  (315, 17)\t1\n",
      "  (316, 43)\t1\n",
      "  (317, 236)\t1\n",
      "  (318, 121)\t1\n",
      "  (319, 213)\t1\n",
      "  (320, 190)\t1\n",
      "  (321, 22)\t1\n",
      "  (322, 92)\t1\n",
      "  (323, 117)\t1\n",
      "  (325, 82)\t1\n",
      "  (326, 252)\t1\n",
      "  (326, 234)\t1\n",
      "  (327, 236)\t1\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorize = CountVectorizer()\n",
    "print(vectorize.fit_transform(words_clean[17:1000:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', \"'s\", \"n't\", 'quot', \"'m\", 'good', 'like', 'get', 'lol', 'u', 'know', 'love', 'thanks', 'one', 'go', 'got', 'day', 'see', \"'ll\", 'amp', 'you', 'well', 'http', 'time', \"'re\", 'oh', 'think', 'it', 'im', 'haha', 'really', 'going', 'hope', 'would', 'work', 'ca', 'sorry', 'still', 'back', 'want', 'yeah', 'much', 'today', 'great', 'na', 'miss', 'need', \"'ve\", '2', 'could', 'right', 'new', 'yes', 'twitter', 'though', 'night', 'come', 'the', 'hey', 'fun', 'make', 'last', 'that', 'better', 'thank', 'feel', 'wish', 'sad', '3', 'lt', 'nice', 'bad', 'way', 'but', 'home', 'happy', 'and', 'never', 'ur', 'sure', 'morning', 'say', 'awesome', 'even', 'my', 'always', 'dont', 'wait', 'people', '..', 'no', 'us', 'gon', 'take', 'we', 'soon', 'next', 'let', 'week', 'tomorrow', \"'d\", 'cool', 'so', 'ok', 'show', 'thing', 'please', 'look', 'guys', 'something', 'follow', 'hear', 'getting', 'tonight', 'ya', 'glad', 'tell', 'find', 'yet', 'maybe', 'best', 'little', 'x', 'wan', 'just', 'thats', 'thought', 'man', 'cant', 'hate', 'what', 'how', 'already', 'have', 'sleep', 'long', '4', 'sounds', 'first', 'pretty', 'watch', 'keep', 'try', 'days', 'help', 'do', 'made', 'not', 'hi', 'said', 'welcome', 'tweet', 'life', 'girl', 'aww', 'weekend', 'everyone', 'things', 'actually', 'ever', 'also', 'sucks', 'working', 'guess', 'went', 'lot', 'done', 'missed', 'omg', 'might', 'wo', 'give', 'live', 'looking', 'away', 'phone', 'luck', 'looks', 'bit', 'friend', 'saw', 'song', 'a', 'makes', 'mean', 'yay', 'stuff', 'year', 'school', 'wow', 'big', 'old', 'anything', 'nothing', 'me', 'amazing', 'coming', 'followfriday', 'someone', 'hahaha', 'gt', 'bed', 'lost', 'least', 'use', 'wanted', 'friends', 'around', 'if', 'must', 'mine', 'another', 'call', 'talk', 'funny', 'movie', 'many', 'hard', 'enjoy', 'cute', 'read', 'n', 'since', 'left', 'r', 'okay', 'damn', 'trying', 'check', 'he', 'watching', 'ha', 'may', 'feeling', 'following', 'two', 'tho', 'put', 'later', 'they', 'house', 'awww', 'sick', '1', 'baby', 'poor', 'play', 'start', 'everything', 'music', 'either', 'ta', 'totally', 'birthday', 'true', 'real', 'now', 'is', 'far', 'free', 'hot', 'summer', 'didnt', 'sweet', 'job', 'seen', 'name', 'pic', 'friday', 'followers', 'send', 'yesterday', 'this', 'wrong', 'late', 'mom', 'times', 'xx', 'tweets', 'world', 'game', 'video', 'found', 'enough', 'excited', 'idea', 'probably', 'hehe', 'stop', 'every', 'iphone', 'without', 'early', 'hours', 'god', 'leave', 'aplusk', 'end', 'win', 'your', 'cause', 'party', 'fine', 'ill', 'why', 'thinking', 'gone', 'beautiful', 'dude', 'using', 'place', 'btw', 'head', 'anymore', 'hello', 'used', 'heard', 'buy', 'remember', 'jealous', 'weather', 'shit', 'add', 'stay', 'making', 'to', 'kind', 'lucky', 'congrats', 'lmao', 'link', 'tired', 'years', 'ah', 'able', 'course', 'told', 'pay', 'tried', 'forward', 'waiting', 'meet', 'yea', 'believe', 'rain', 'till', 'anyway', 'seems', 'money', 'b', 'aw', 'almost', 'she', '5', 'ago', 'car', 'did', 'its', 'kinda', 'eat', 'lovely', 'post', 'mind', 'ready', 'will', 'pics', 'sunday', 'busy', 'person', 'else', 'are', 'xd', 'quite', 'guy', 'fan', 'finally', 'hopefully', 'news', 'problem', 'picture', 'all', 'crazy', 'agree', 'family', 'food', 'lots', 'there', 'cuz', 'talking', 'hair', 'definitely', 'face', 'called', 'yep', 'came', 'ask', 'weeks', 'in', 'care', 'sun', 'bring', 'reply', 'loved', 'part', 'stupid', 'book', 'heart', 'coffee', '100', 'blog', 'kids', 'seeing', 'monday', 'site', 'playing', 'anyone', 'email', 'missing', 'gets', 'happened', 'online', 'tv', 'whole', 'wont', 'ugh', 'story', 'forgot', 'sometimes', 'says', 'cold', 'w/', 'started', 'boo', 'bored', 'train', 'room', 'super', 'rest', 'change', 'means', 'sound', 'ones', 'full', 'took', 'hell', 'sooo', 'alexalltimelow', '10', 'dad', 'run', 'together', 'trip', 'at', 'goes', 'dm', 'sent', 'thx', 'hour', 'unfortunately', 'girls', 'ashleytisdale', 'happen', 'lunch', 'list', 'seriously', 'very', 'knew', 'hang', 'andyclemmensen', 'word', 'works', 'nope', 'soo', 'wonderful', 'listen', 'dear', 'hit', 'favorite', 'was', 'reading', 'when', 'rock', '*sigh*', 'xxx', 'facebook', 'for', 'reason', 'saturday', 'boy', 'bet', 'cry', 'suck', 'month', 'taking', 'ff', 'ppl', 'chance', 'too', 'hurt', 'on']\n"
     ]
    }
   ],
   "source": [
    "most_freq_words = FreqDist(words_clean).most_common()[:500] #500 gives good results, 1000 is much slower and give +1% compparing to 500\n",
    "features = []\n",
    "for word in most_freq_words:\n",
    "    features.append(word[0])\n",
    "    \n",
    "    \n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_features(tweet, word_features):\n",
    "    words = word_tokenize(tweet)\n",
    "    words = clean_tokenz(words)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "\n",
    "    return features\n",
    "\n",
    "featureset =  [(find_features(tweet, features), sentiment) for tweet, sentiment in zip(tweet_strings_list,results_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(len(featureset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for classifier: <nltk.classify.naivebayes.NaiveBayesClassifier object at 0x00000213EB65F2B0> is 71.092000\n",
      "Accuracy for classifier: <SklearnClassifier(LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False))> is 72.084000\n",
      "Accuracy for classifier: <SklearnClassifier(KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform'))> is 65.216000\n",
      "Classsifer <SklearnClassifier(LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False))> has the best score\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import random\n",
    "\n",
    "#validation phase\n",
    "random.shuffle(featureset)\n",
    "training_set = featureset[:50000]\n",
    "validation_set = featureset[50000:75000]\n",
    "test_set = featureset[75000:]\n",
    "\n",
    "classifiers = []\n",
    "classifiers.append(nltk.classify.NaiveBayesClassifier)\n",
    "classifiers.append(nltk.classify.SklearnClassifier(LogisticRegression()))\n",
    "classifiers.append(nltk.classify.SklearnClassifier(KNeighborsClassifier()))\n",
    "trained_classifers=[]\n",
    "                   \n",
    "for classifier in classifiers:\n",
    "     trained_classifers.append(classifier.train(training_set))\n",
    "    \n",
    "#   classifier.show_most_informative_features(15)\n",
    "\n",
    "accuaracy_tuples =[]\n",
    "max_accuracy = 0\n",
    "best_classifier = None\n",
    "for classifier in trained_classifers:\n",
    "    \n",
    "    accuracy = nltk.classify.accuracy(classifier, validation_set)*100\n",
    "    print('Accuracy for classifier: %s is %f' % (str(classifier), accuracy))\n",
    "      \n",
    "    if accuracy > max_accuracy:\n",
    "        max_accuracy = accuracy\n",
    "        best_classifier = classifier\n",
    "        \n",
    "print(\"Classsifer %s has the best score\" % str(best_classifier))\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy percent: 70.95922205770539\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Test set accuracy percent:\", (nltk.classify.accuracy(best_classifier, test_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
